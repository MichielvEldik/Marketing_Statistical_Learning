---
title: "Moving Average model"
output: html_document
---
***
## __1. Conceptual Background__
### __1.1. MA(q) Model__
An __MA(q)__ model forecasts the outcome variable $Y_T$ based solely on the past errors in the series - called __error lags__. Last month you made an estimation of sales amount. The degree to which you were wrong last month will be used to estimate the sales amount for next month. The sales amount itself is not used at all in our predictions. This is one of the main characteristics that sets MA(q) apart from [__AR(q)__](AR_Model.html) models. This story can be summarized in the following equation for an __MA(q)__ model in its most basic form, the __MA(1)__ model. 

$$Y_t = \mu + \theta\varepsilon_{t-1} + \varepsilon_t$$

* $Y_t$ = Prediction of the outcome variable at lag $t$
* $Y_{t-1}$ = the difference between yesterday's predicted and actual value of $Y_t$ (i.e. 'yesterday's wrongness')
* $\theta$ = The weight of yesterday's wrongness in our prediction of $Y_t$.
* $\mu$ = The mean of the time series. Oftentimes it's set at 0. 
* $\varepsilon_t$ = Today's wrongness, which of course we don't know yet and can only fill in after discovering $Y_t$.   

#### __1.1.2. The meaning of 'error'__
But what does an error really mean? Well let's pretend that you have two lines in a time series. One line is your predicted Sales. One is the actual sales made. Essentially, it is how wrong you were yesterday, that is affecting the prediction of tomorrow. The error reflects some sort of external shock due to some hidden factors (in the sense that they are not included in the model). Essentially, you can think of lagged errors as one long term effect minus some slightly less long term effect which gives us a short term effect.

#### __1.1.3. How can one predict the first observation if there was no error?__
If you have no prediction for the first observation, how can you start? Well, typically you would start out with the mean of the time series. *If your time series is long enough, it really won't matter where your starting point is*.


### __1.2. The 'Short Memory Model'__
One of the reasons why we shouldn't care about where to begin is the fact that MA is a *short memory model*. This means that error's don't last long in the future.

* In AR(1) models, the effect of shocks that happen long ago have little effect on the present if $|\theta| < 1$. 
* In MA(1) models, the effect of shocks have *NO* effect on the present if they happened long enough ago.

Let's have a look at equations for yesterday, today, and tomorrow prediction of sales:

$$Y_t = \mu + \theta\varepsilon_{t-2} + \varepsilon_t$$
$$Y_t = \mu + \theta\varepsilon_{t-1} + \varepsilon_t$$
$$Y_{t+1} = \mu + \theta\varepsilon_1 + \varepsilon_{t+1}$$

As you can see, the error from yesterday has *no part* in predicting the error for today. It's basically going back to this idea of stationarity. Stationarity was the idea that the dependence of previous observations declines over time. In MA processes, dependence of previous observations actually *disappears* as long as you go far enough into the future.

With Autoregressive models if we look back in time, the first observation still maters (even if this is a very tiny amount). For MA models, there will be literally *NO* effect if you go back in time far enough.

### __1.3. Some facts on MA processes__
An MA process...

* ACF has only one spike
* PACF decays exponentially
* MA(1) is equivalent to AR($\infty$) if $|\theta| < 1$
* No stationarity restriction required
* Mean is time-independent. Series return to their historical level
* variance is time-independent. Series return to their historical level

### __1.4. Reasons for MA behaviour in marketing__
There can be either a positive or negative MA behavior in marketing. __Postive behavior__: Higher beer sales today due to unexpected heat wave, but also likely tomorrow as heat wave stays on. __Negative relation__: Equilibrium-seeking behaviour of supply and demand such as natural disaster, coronavirus, et cetera. 

### __1.5. When is an MA($q$) Process Present and at what order?__
Contrary to __AR($q$) Processes__, a purely __MA($q$)__ process can be identified using the autocorrelation function. A purely __MA($q$)__ process is a process in which _no future value can be predicted by its previous value_. Think of it like the baseline of a pure __MA($q$)__ process is white noise with one 'random' shock that deviates substantially and significantly from this white noise series. This means that for the one lag in which such a shock occurs, due to the local variance there will actually be significant autocorrelation. Thus we can state that for an $MA(q)$, $\rho_k=0$ for all $k > q$. 


### __1.6. Why and How an AR(1) Model is an MA($\infty$) Model__
Invertibility requires $0 < \varphi < 1$. 

_More to come..._

***

## __2. Case example__
### __2.1. Plotting the Time Series Data__
```{r library, message=FALSE, results='hide', echo=FALSE}
library(FitAR)
library(forecast)
library(tseries)
library(gplots)
library(aTSA)
library(astsa)
```

```{r read file, echo=FALSE}
food.df <- read.csv("../SLIM0405DataSeries.csv")
```

```{r, fig.align = 'center'}
plot(food.df[,c(1)],food.df[,c(3)], type="l", col="red", lwd=1, xlab="weeks", ylab="sales", main="Volume sales over time")
```
### __2.2. Testing for Stationarity__
Not a requirement per se.
#### __2.2.1. Augmented Dickey Fuller Test__

```{r }
adf.test(food.df[,c(3)])
```

#### __2.2.2. determining type and order with ACF & PACF__
We can recognize MA(1) by observing the ACF and PACF.

* Theoretical ACF has only one spike
* Theoretical PACF has exponential decay
The exponential decay can go in two ways: An MA(1), $0 < \theta < 1$ will damp out. MA(1), $-1 < \theta < 0$ will change in signal.

```{r }
foodacf = acf(food.df[,c(3)], lag.max = 10)
```


```{r }
foodpacf = pacf(food.df[,c(3)], lag.max = 10)
```

```{r, echo=FALSE }
bartry = Reduce(rbind,foodacf)
bartry2 = t(bartry)
bartry3 = as.numeric(bartry2[,c(1)])
bartryb = Reduce(rbind,foodpacf)
bartry2b = t(bartryb)
bartry3b = as.numeric(bartry2b[,c(1)])

bartry4 = as.matrix(bartry3)
bartry4help = t(bartry4)
bartry4help = bartry4help[,c(2,3,4,5,6,7,8,9,10,11)]
bartry4 = t(bartry4help)
bartry4 = t(bartry4)
bartry4b = as.matrix(bartry3b)
bartry5 = cbind(bartry4,bartry4b)
bartry6 = t(bartry5)
```


```{r }
barplot(bartry6, ylim = c(-1,1), main = "ACF (red) and PACF (grey) function", col = c("red", "grey"), beside = TRUE)
```



#### __auto.arima test__
```{r }
foodsel = food.df[,c(3)]
tsfoodsel = as.ts(foodsel)
auto.arima(tsfoodsel, test = c("adf"), trace = TRUE)
```

